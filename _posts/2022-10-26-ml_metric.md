---
title: '머신러닝 개념 - 척도(metric): accuracy, precision, recall, f1 score'
date: 2022-10-26
categories:
- ML
tag:
- ML
- metric
---

척도는 모델을 평가하기 위해 사용하는 값입니다. 척도를 통해 모델의 성능을 비교하여 여러 모델 중 하나를 선택할 수도 있고, 기존 모델의 성능이 낮다면 이를 높이기 위해 다양한 대처를 해볼 수도 있습니다. 결국 잘 맞추면 되는 것 아닌가? 즉 정확도(accuracy)가 높으면 되는 것 아닌가? 라고 생각할 수도 있지만, **데이터의 특징이나 상황, 목적에 따라** 다양한 척도가 사용될 수 있습니다.

척도에 대한 설명으로 바로 들어가기 전에, 분류 문제에 있어서 다음과 같은 confusion matrix를 그려볼 수 있는데요,

|  |    | Predicted/classified label   예측된/분류된 라벨 |>|
| --- | --- | --- | --- |
|     |     | Negative | Positive |
|Actual label   실제 라벨| Negative | <p style="text-align: center;">True Negative</p> <p style="text-align: center;">980</p>| <p style="text-align: center;">False Positive</p>0 |
|^                     |Positive | <p style="text-align: center;">False Negative</p> 10 | <p style="text-align: center;">True Positive</p> 10 |

우선, 단순히 답을 얼마나 맞췄는지(negative인 것을 negative라고 분류, positive인 것을 positive라고 분류)를 정확도라고 정의하고 생각해봅시다. 위의 정보를 통해, 모델의 정확도는 (980+10)/(980+10+10) = **0.99**라는 것을 알 수 있죠! 매우 높은 정확도입니다.

그러나 살펴보면, 이 모델이 실제로 negative인 것에 대해서는 모두 negative라고 정확하게 예측했지만, positive인 것에 대해서는 20개 중 10개를, 즉 1/2의 확률로 positive라고 예측했다는 것을 알 수 있습니다. <u>즉 실제 positive인 것에 대한 예측률은 낮은 편이죠.</u>

나아가, 애초에 전체 데이터 셋에서 negative 인 데이터이 980개, positive인 데이터가 20개 뿐입니다. **데이터의 분포가 상당히 불균형합니다.** 이 경우, 그냥 모든 데이터에 대해 negative라고 예측하는 멍청한(!) 모델을 만들어도 그 모델은 0.98의 정확도를 갖게 됩니다. 

이처럼 단순하게 정확도만 계산해서는 모델의 성능을 제대로 평가할 수 없으므로, 이를 위한 다양한 척도가 사용되고 있습니다.

#### 정확도(Accuracy)

앞서 잠깐 설명했듯이, 정확도가 가장 단순한 척도입니다. true label을 얼마나 맞추었는지를 계산합니다.

$$Accuracy = \frac{True \: Positive+True \: Negative}{True\:Positive+False\:Positive+True\:Negative+False\:Negative}$$

#### 정밀도(Precision)

모델이 positive로 예측/분류한 것 중 얼마나 실제로 positive인지 나타내는 척도로, 양성예측률(positive predictive value)라고도 불립니다.

$$Precision = \frac{True \: Positive}{True\:Positive+False\:Positive}$$

식을 들여다보면, 정밀도는 false positive가 작으면 높아집니다. 즉, 정밀도는 모델을 활용하는 맥락을 생각해볼 때, <strong><span style="background-color: #fff5b1">false positive의 비용이 매우 높아 false positive를 줄여야할 때</span></strong> 사용하기 좋습니다. <u><strong>스팸 메일을 분류하는 문제</strong></u>를 생각해볼 수 있죠. 이 상황에서 false positive는 스팸이 아닌(negative) 메일이 스팸으로 분류되는 것입니다. 모델의 정밀도가 높지 않은 경우 중요한 이메일이 스팸으로 분류될 수 있고, 그렇게 되면 사용자가 중요한 메일을 놓치는 상황이 생길 수 있습니다. 이에 이러한 경우처럼 false positive가 최소화되어야하는 경우에는 정밀도를 사용하면 좋습니다.

#### 재현율(Recall)

실제 positive 중에서 우리의 모델이 얼마나 positive라고 잡아냈는지 나타내는 척도이며, 민감도(sensitivity)라고도 불립니다.

$$Recall=\frac{True\:Positive}{True\:Positive+False\:Negative}$$

정밀도에서 살펴본 것처럼 식을 들여다보면, 재현율은 false negative가 작으면 높아집니다. 즉 <strong><span style="background-color: #fff5b1">false negative를 최소화해야 할 때</span></strong>사용하기 좋습니다. <u><strong>의료 현장</strong></u>, 즉 <u><strong>환자의 질환 진단 여부를 분류/예측하는 문제</u></strong>를 생각해볼 수 있겠죠. 환자가 실제로 병을 갖고 있는데 (positive), 병이 없는 것으로 분류/예측되는 경우로, 만약 해당 병이 빠른 조치가 필요한 종류의 병이었다면, 잘못된 예측의 결과가 매우 치명적일 수 있습니다. 

#### F1 score

F1 score는 정밀도와 재현율을 함께 고려한 척도입니다.

$$F1=2\times \frac{Precision*Recall}{Precision+Recall}$$

즉, false negative와 false positive를 모두 최소화하는 것이 중요하여 <strong><span style="background-color: #fff5b1">재현율과 정밀도 간의 균형/절충이 필요한 경우</span></strong> 사용될 수 있습니다. 동시에 정확도와는 달리 <strong><span style="background-color: #fff5b1">전체 데이터 분포가 불균형할 때</span></strong>에도 사용될 수 있습니다. 그러나 f1도 데이터의 불균형을 완벽히 보정해주지는 못하고 특히 true negative가 고려되지 않는다는 한계가 있습니다. 또한, 앞서 설명했듯 문제 상황에 따라 재현율과 정밀도의 상대적인 중요도가 다를 수 있는데, 두 척도에 같은 중요도를 부여한다는 한계도 존재합니다. 그러나 여전히 널리 쓰이는 척도입니다.

참고한 자료

- [https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)

- [https://en.wikipedia.org/wiki/F-score](https://en.wikipedia.org/wiki/F-score)

